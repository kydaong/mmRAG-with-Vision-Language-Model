{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4316f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "993a0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, output_dir: str = \"data/processed\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.images_dir = self.output_dir / \"images\"\n",
    "        self.images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text and embedded images using PyMuPDF\n",
    "        More reliable than unstructured + poppler\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        filename = Path(pdf_path).stem\n",
    "        \n",
    "        try:\n",
    "            # Open PDF with PyMuPDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                \n",
    "                # Extract text from page\n",
    "                text = page.get_text()\n",
    "                \n",
    "                # Split text into reasonable chunks (by paragraphs)\n",
    "                paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "                \n",
    "                for i, para in enumerate(paragraphs):\n",
    "                    if len(para) > 50:  # Skip very short text\n",
    "                        documents.append({\n",
    "                            'type': 'text',\n",
    "                            'content': para,\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'page': page_num + 1,\n",
    "                                'chunk_id': f\"{filename}_p{page_num}_text_{i}\",\n",
    "                            }\n",
    "                        })\n",
    "                \n",
    "                # Extract embedded images from page\n",
    "                image_list = page.get_images(full=True)\n",
    "                \n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    try:\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save image\n",
    "                        image_filename = f\"{filename}_p{page_num}_img{img_index}.{image_ext}\"\n",
    "                        image_path = self.images_dir / image_filename\n",
    "                        \n",
    "                        with open(image_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        documents.append({\n",
    "                            'type': 'image',\n",
    "                            'content': str(image_path),\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'page': page_num + 1,\n",
    "                                'chunk_id': f\"{filename}_p{page_num}_img{img_index}\",\n",
    "                            }\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: Could not extract image {img_index} from page {page_num}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {filename}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_directory(self, directory: str = \"data/raw\", \n",
    "                         recursive: bool = False) -> List[Dict]:\n",
    "        \"\"\"Process all PDFs in a directory\"\"\"\n",
    "        all_documents = []\n",
    "        \n",
    "        if recursive:\n",
    "            pdf_files = list(Path(directory).rglob(\"*.pdf\"))\n",
    "        else:\n",
    "            pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            print(f\"⚠️  No PDF files found in {directory}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "        \n",
    "        for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            docs = self.process_pdf(str(pdf_file))\n",
    "            \n",
    "            # Show progress for each file\n",
    "            text_count = sum(1 for d in docs if d['type'] == 'text')\n",
    "            img_count = sum(1 for d in docs if d['type'] == 'image')\n",
    "            print(f\"  ✓ {pdf_file.name}: {text_count} text chunks, {img_count} images\")\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "        \n",
    "        return all_documents\n",
    "    \n",
    "    def save_to_json(self, documents: List[Dict], \n",
    "                     output_file: str = \"data/processed/documents.json\"):\n",
    "        \"\"\"Save extracted documents to JSON\"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(documents, f, indent=2)\n",
    "        print(f\"\\n✅ Saved {len(documents)} documents to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96c881ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, output_dir: str = \"data/processed\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.images_dir = self.output_dir / \"images\"\n",
    "        self.images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"Extract text AND embedded images from a single PDF\"\"\"\n",
    "        \n",
    "        documents = []\n",
    "        filename = Path(pdf_path).stem\n",
    "        \n",
    "        try:\n",
    "            elements = partition_pdf(\n",
    "                filename=pdf_path,\n",
    "                extract_images_in_pdf=True,\n",
    "                image_output_dir_path=str(self.images_dir),\n",
    "                infer_table_structure=True,\n",
    "                chunking_strategy=\"by_title\",\n",
    "                max_characters=4000,\n",
    "                new_after_n_chars=3800,\n",
    "                combine_text_under_n_chars=2000,\n",
    "            )\n",
    "            \n",
    "            for i, element in enumerate(elements):\n",
    "                if element.category in [\"Title\", \"NarrativeText\", \"Text\", \"ListItem\"]:\n",
    "                    if element.text.strip():\n",
    "                        documents.append({\n",
    "                            'type': 'text',\n",
    "                            'content': element.text,\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'chunk_id': f\"{filename}_text_{i}\",\n",
    "                                'element_type': element.category\n",
    "                            }\n",
    "                        })\n",
    "                \n",
    "                elif element.category == \"Image\":\n",
    "                    image_path = element.metadata.image_path\n",
    "                    if image_path:\n",
    "                        documents.append({\n",
    "                            'type': 'image',\n",
    "                            'content': image_path,\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'chunk_id': f\"{filename}_img_{i}\",\n",
    "                            }\n",
    "                        })\n",
    "                \n",
    "                elif element.category == \"Table\":\n",
    "                    documents.append({\n",
    "                        'type': 'table',\n",
    "                        'content': element.text,\n",
    "                        'metadata': {\n",
    "                            'source': filename,\n",
    "                            'chunk_id': f\"{filename}_table_{i}\",\n",
    "                        }\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {filename}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_directory(self, directory: str = \"data/raw\", \n",
    "                         recursive: bool = False) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Process all PDFs in a directory\n",
    "        \n",
    "        Args:\n",
    "            directory: Path to folder with PDFs\n",
    "            recursive: If True, searches subdirectories too\n",
    "        \"\"\"\n",
    "        all_documents = []\n",
    "        \n",
    "        # Get PDF files\n",
    "        if recursive:\n",
    "            pdf_files = list(Path(directory).rglob(\"*.pdf\"))  # Recursive\n",
    "        else:\n",
    "            pdf_files = list(Path(directory).glob(\"*.pdf\"))   # Single folder\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "        \n",
    "        # Process each PDF with progress bar\n",
    "        for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            docs = self.process_pdf(str(pdf_file))\n",
    "            all_documents.extend(docs)\n",
    "        \n",
    "        return all_documents\n",
    "    \n",
    "    def save_to_json(self, documents: List[Dict], \n",
    "                     output_file: str = \"data/processed/documents.json\"):\n",
    "        \"\"\"Save extracted documents to JSON\"\"\"\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(documents, f, indent=2)\n",
    "        print(f\"\\n✅ Saved {len(documents)} documents to {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processor = DocumentProcessor()\n",
    "    documents = processor.process_directory(\"data/raw\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Text chunks: {sum(1 for d in documents if d['type'] == 'text')}\")\n",
    "    print(f\"Embedded images: {sum(1 for d in documents if d['type'] == 'image')}\")\n",
    "    print(f\"\\nImages location: {processor.images_dir}\")\n",
    "    \n",
    "    if documents:\n",
    "        processor.save_to_json(documents)\n",
    "    else:\n",
    "        print(\"\\n⚠️  No documents extracted. Check your PDF files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d322f37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  33%|███▎      | 1/3 [00:36<01:12, 36.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR processing Exxon_OIMS_framework: Unable to get page count. Is poppler installed and in PATH?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:  67%|██████▋   | 2/3 [01:05<00:32, 32.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR processing Risk_Based_Inspection__RBI__580: Unable to get page count. Is poppler installed and in PATH?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs: 100%|██████████| 3/3 [01:09<00:00, 23.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR processing Vertical turbine pump IOM: Unable to get page count. Is poppler installed and in PATH?\n",
      "\n",
      "============================================================\n",
      "EXTRACTION SUMMARY\n",
      "============================================================\n",
      "Total documents: 0\n",
      "Text chunks: 0\n",
      "Embedded images: 0\n",
      "Tables: 0\n",
      "\n",
      "✅ Saved 0 documents to data/processed/documents.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processor = DocumentProcessor()\n",
    "    #documents = processor.process_directory(\"data/raw\")\n",
    "    documents = processor.process_directory(\"D:/Projects/mmRAG-with-Vision-Language-Model/PDFs\")\n",
    "    #documents = processor.process_directory(\"PDFs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Text chunks: {sum(1 for d in documents if d['type'] == 'text')}\")\n",
    "    print(f\"Embedded images: {sum(1 for d in documents if d['type'] == 'image')}\")\n",
    "    print(f\"Tables: {sum(1 for d in documents if d['type'] == 'table')}\")\n",
    "    \n",
    "    processor.save_to_json(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9584be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
