{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4316f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.auto import partition\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import uuid\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "993a0259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    def __init__(self, output_dir: str = \"data/processed\"):\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.images_dir = self.output_dir / \"images\"\n",
    "        self.images_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "    def process_pdf(self, pdf_path: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract text and embedded images using PyMuPDF\n",
    "        More reliable than unstructured + poppler\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        filename = Path(pdf_path).stem\n",
    "        \n",
    "        try:\n",
    "            # Open PDF with PyMuPDF\n",
    "            doc = fitz.open(pdf_path)\n",
    "            \n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                \n",
    "                # Extract text from page\n",
    "                text = page.get_text()\n",
    "                \n",
    "                # Split text into reasonable chunks (by paragraphs)\n",
    "                paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "                \n",
    "                for i, para in enumerate(paragraphs):\n",
    "                    if len(para) > 10:  # Skip very short text\n",
    "                        documents.append({\n",
    "                            'type': 'text',\n",
    "                            'content': para,\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'page': page_num + 1,\n",
    "                                'chunk_id': f\"{filename}_p{page_num}_text_{i}\",\n",
    "                            }\n",
    "                        })\n",
    "                \n",
    "                # Extract embedded images from page\n",
    "                image_list = page.get_images(full=True)\n",
    "                \n",
    "                for img_index, img in enumerate(image_list):\n",
    "                    try:\n",
    "                        xref = img[0]\n",
    "                        base_image = doc.extract_image(xref)\n",
    "                        image_bytes = base_image[\"image\"]\n",
    "                        image_ext = base_image[\"ext\"]\n",
    "                        \n",
    "                        # Save image\n",
    "                        image_filename = f\"{filename}_p{page_num}_img{img_index}.{image_ext}\"\n",
    "                        image_path = self.images_dir / image_filename\n",
    "                        \n",
    "                        with open(image_path, \"wb\") as img_file:\n",
    "                            img_file.write(image_bytes)\n",
    "                        \n",
    "                        documents.append({\n",
    "                            'type': 'image',\n",
    "                            'content': str(image_path),\n",
    "                            'metadata': {\n",
    "                                'source': filename,\n",
    "                                'page': page_num + 1,\n",
    "                                'chunk_id': f\"{filename}_p{page_num}_img{img_index}\",\n",
    "                            }\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"  Warning: Could not extract image {img_index} from page {page_num}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            doc.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {filename}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def process_directory(self, directory: str = \"data/raw\", \n",
    "                         recursive: bool = False) -> List[Dict]:\n",
    "        \"\"\"Process all PDFs in a directory\"\"\"\n",
    "        all_documents = []\n",
    "        \n",
    "        if recursive:\n",
    "            pdf_files = list(Path(directory).rglob(\"*.pdf\"))\n",
    "        else:\n",
    "            pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "        \n",
    "        if not pdf_files:\n",
    "            print(f\"⚠️  No PDF files found in {directory}\")\n",
    "            return []\n",
    "        \n",
    "        print(f\"Found {len(pdf_files)} PDF files\\n\")\n",
    "        \n",
    "        for pdf_file in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "            docs = self.process_pdf(str(pdf_file))\n",
    "            \n",
    "            # Show progress for each file\n",
    "            text_count = sum(1 for d in docs if d['type'] == 'text')\n",
    "            img_count = sum(1 for d in docs if d['type'] == 'image')\n",
    "            print(f\"  ✓ {pdf_file.name}: {text_count} text chunks, {img_count} images\")\n",
    "            \n",
    "            all_documents.extend(docs)\n",
    "        \n",
    "        return all_documents\n",
    "    \n",
    "    def save_to_json(self, documents: List[Dict], \n",
    "                     output_file: str = \"data/processed/documents.json\"):\n",
    "        \"\"\"Save extracted documents to JSON\"\"\"\n",
    "        output_path = Path(output_file)\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(documents, f, indent=2)\n",
    "        print(f\"\\n✅ Saved {len(documents)} documents to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    processor = DocumentProcessor()\n",
    "    documents = processor.process_directory(\"D:/Projects/mmRAG-with-Vision-Language-Model/PDFs\")\n",
    "\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    #print(\"=\"*60)\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Text chunks: {sum(1 for d in documents if d['type'] == 'text')}\")\n",
    "    print(f\"Embedded images: {sum(1 for d in documents if d['type'] == 'image')}\")\n",
    "    print(f\"\\nImages location: {processor.images_dir}\")\n",
    "    \n",
    "    if documents:\n",
    "        processor.save_to_json(documents)\n",
    "    else:\n",
    "        print(\"\\n⚠️  No documents extracted. Check your PDF files.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9584be4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
